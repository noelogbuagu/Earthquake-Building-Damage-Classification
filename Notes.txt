4.1

Focusing on data from 2015 Nepal Earthquake.
The goal is to create a predictive model that when given the characteristics of a building can predict whether that building with suffer severe damage in a similar type earthquake.
What we'll be doing:
    Data wrangling with SQL
        connect to sqlite server
        explore database and create dataset
        import database query into dataframe

4.2 

After the query is imported into a dataframe, all leaky features are dropped.
These leaky features are information that would not exist before the earthquake happens.
So all post earthquake features need to be removed from the dataframe.

Since this logistic regression is still a type of linear model, despite it being used for classification, multicollinearity needs to be reduced
that is, you can't have numeric variable being highly correlated with each other 
Then, out of the correlated pairs, the variable that is least correlated with the target variable will be removed.

Whenever i am doing a classification problem, you need to take a look at the balance/proportion of the 2 classefa-spin
the positive class in a classification problem is the one denoted with 1, negative class is denoted as 0
the majority class is the class with the higher proportion.

For the baseline of a classification problem, there is only one guess, A or B.
That one guess would be the majority class because you'd be right more times than not
So what would be the accuracy score for a model that always guessed the majority class?
That would b ethe best way to think about it.
The accuracy score has to be greater than the baseline accuracy score for the model to be useful.

we need to create a model that doesn't go below 0 or above 1.
whatever comes out of the function that we create should be treated ass a step to making that prediction.

predict probability lets you know how certain the predictions are.

Odds ratio basically tells you the chances of a positive class happening.

4.3

the data will be split into a train, validation and test sets.
The model learns from the training data but has no access to the validation and test
the data scientist has access to the training set and validation but not the test set.
the training set is for training the model 
the validation set is for tuning the model
the test set is for testing the model
the validation set allows you to iterate, try differnt things till you are comfortable enough with the performance of the model to test it with the test set

Built a decision tree model
tuned hyperparameters
explain predicitions using gini importance


4.4

Performed a four table join for household demographic table
reduced a high cardinality variable
created a logistic regression model to predict severe damage
explain the model's predictions using odds ratios 
